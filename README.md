# Audio Spectrogram Transformer (AST) for speech
Adapting the Audio Spectrogram Transformer (AST) for human language classification.
![image](CS_224S_Poster.png)

Class project for Stanford CS 224S (Spoken Language Processing), spring 2024.

# Notebook Pointers
Please refer to the below links to see how AST was finetuned and evaluated for various spoken language tasks.

## AST for ASR
* [Google FLEURS](https://github.com/poojasethi/ast-speech/blob/main/Automatic_Speech_Recognition_(FLEURS_Further_Extended_Training).ipynb)
* [Librispeech](https://github.com/poojasethi/ast-speech/blob/main/Automatic_Speech_Recognition_(Librispeech).ipynb)

## AST for Classification
* [Emotion Recognition - IEMOCAP](https://github.com/poojasethi/ast-speech/blob/main/emotion_classification_IEMOCAP.py)
* [Emotion Classification - RAVDESS](https://github.com/poojasethi/ast-speech/blob/main/emotion_classification_RAVDESS.py)
* [Music Genre Classification](https://github.com/poojasethi/ast-speech/blob/main/music_genre_classification_GTZAN.py)

## Spectrogram and Attention Map Visualization
* [Attention Map Visualizations](https://github.com/poojasethi/ast-speech/blob/main/visualizations.ipynb)

